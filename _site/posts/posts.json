[
  {
    "path": "posts/2021-11-16-mademo/",
    "title": "Meta-analysis demo in metafor",
    "description": "In this blog I'll be walking you through the basic steps of a meta-analysis using metafor.",
    "author": [
      {
        "name": "Austin Gallyer",
        "url": {}
      }
    ],
    "date": "2021-11-16",
    "categories": [],
    "contents": "\r\nBackground\r\nI’m TA’ing for a meta-analysis course and the final tutorial I gave at the end of the course was to put together everything we had so far to conduct a meta-analysis from start (e.g., importing the data) to finish (e.g., creating publication ready figures).\r\nInfo about the data\r\nThis is a fake data set (created by Rick Wagner, the instructor for the meta-analysis course at FSU) consisting of 20 studies with a between-group design. In this case, the story behind the data set is that these are studies examining some kind of intervention for reading.\r\nImporting the data\r\nFirst we will load the packages we need. If you don’t have metafor or the tidyverse go ahead and run install.packages for both of those.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(metafor)\r\n\r\n\r\n\r\nNext, we will import the data. Because the data set is in the same directory as my working directory, along with the code, I don’t need to specify the path.\r\n\r\n\r\ndat <- read_csv('metafordata.csv')\r\n\r\n\r\n\r\nIt is always a good idea to take a look at your data after you read it in to make sure it looks like you expect it to look. To do that, I’m going to use the glimpse function.\r\n\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 20\r\nColumns: 8\r\n$ Study   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1~\r\n$ EXPmean <dbl> 103.2, 105.7, 107.2, 110.4, 109.6, 106.9, 102.4, 123~\r\n$ EXPsd   <dbl> 5.5, 4.8, 4.7, 5.2, 6.0, 5.5, 4.9, 5.1, 6.4, 3.9, 7.~\r\n$ EXPn    <dbl> 50, 60, 40, 35, 450, 40, 45, 60, 30, 320, 37, 46, 43~\r\n$ CONmean <dbl> 100.4, 98.7, 103.2, 105.4, 105.2, 104.3, 106.3, 119.~\r\n$ CONsd   <dbl> 5.5, 4.8, 4.7, 5.2, 6.0, 5.5, 4.9, 5.1, 6.4, 3.9, 7.~\r\n$ CONn    <dbl> 45, 63, 43, 35, 425, 40, 45, 65, 24, 320, 37, 46, 50~\r\n$ grade   <dbl> 5, 2, 4, 3, 3, 6, 4, 4, 1, 5, 2, 4, 3, 3, 1, 1, 3, 4~\r\n\r\nFrom this we can see we have 20 studies (i.e., rows), means, standard deviations, and sample sizes for each of our groups. We also have a moderator variable called grade that shows what grade level the participants in this study were in.\r\nCalculating effect size\r\nNext, we need to calculate an effect size for each of our studies. Because our studies are a between-group design, we are going to use Hedges’ g for our effect size. metafor has a very flexible function called escalc that we can use to calculate this.\r\n\r\n\r\ndat <- escalc(measure = 'SMD', m1i = EXPmean, sd1i = EXPsd, m2i = CONmean, \r\n              sd2i = CONsd, n1i = EXPn, n2i = CONn, data = dat, \r\n              var.names = c('g', 'var_g'))\r\n\r\n\r\n\r\nLet’s walk through the arguments. First, we tell the function what kind of effect size we want. In this case, we are getting a standardized mean difference, or SMD. If you read the metafor documentation, this can be a little bit confusing because in some places it is unclear what kind of effect size this is and in some places it incorrectly says that this calculates Cohen’s d. But if you look carefully, you see that it is Cohen’s d with a correction that turns it into Hedges’ g. The next six arguments specify the column names corresponding to the means, standard deviations, and sample sizes of our groups. By the way we set it up here, Hedges’ g will be calculated, with positive number indicating that our experimental group had a larger mean than the control group. If you wanted the opposite, then you’d want to make sure the control group information goes into the first group arguments (e.g., m1i = CONmean). Next, we give the function our data, and then we give our own variable names for the two numbers this function is going to calculate for us. The defaults are Ti and Vi, corresponding to the effect size and its variance. Because we specify g and var_g, that will be the names of the corresponding columns in our data set.\r\n\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 20\r\nColumns: 10\r\n$ Study   <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1~\r\n$ EXPmean <dbl> 103.2, 105.7, 107.2, 110.4, 109.6, 106.9, 102.4, 123~\r\n$ EXPsd   <dbl> 5.5, 4.8, 4.7, 5.2, 6.0, 5.5, 4.9, 5.1, 6.4, 3.9, 7.~\r\n$ EXPn    <dbl> 50, 60, 40, 35, 450, 40, 45, 60, 30, 320, 37, 46, 43~\r\n$ CONmean <dbl> 100.4, 98.7, 103.2, 105.4, 105.2, 104.3, 106.3, 119.~\r\n$ CONsd   <dbl> 5.5, 4.8, 4.7, 5.2, 6.0, 5.5, 4.9, 5.1, 6.4, 3.9, 7.~\r\n$ CONn    <dbl> 45, 63, 43, 35, 425, 40, 45, 65, 24, 320, 37, 46, 50~\r\n$ grade   <dbl> 5, 2, 4, 3, 3, 6, 4, 4, 1, 5, 2, 4, 3, 3, 1, 1, 3, 4~\r\n$ g       <dbl> 0.5049724, 1.4492722, 0.8431551, 0.9508876, 0.732703~\r\n$ var_g   <dbl> 0.043564312, 0.041077853, 0.052538408, 0.063601337, ~\r\n\r\nVoila! We now have our effect size and its variance for each study.\r\nCalculating average weighted effect size\r\nHere we are! If you were doing a meta-analysis for real you have been spending months, possibly years reading and coding articles now you get to find out what the average weighted effect size is! This is the number that people want to know.\r\n\r\n\r\nre_model <- rma(g ~ 1, var_g, data = dat, method = 'REML')\r\n\r\n\r\n\r\nThis is a random-effects model using restricted maximum-likelihood. I made a few things more explicit in the way I coded it. The first part is r-style formula syntax. g is on the left-hand side of the ~ indicating that it is the dependent variable. But in this case, we only have a 1 on the other side of the ~. This means this is an intercept-only model and thus, we will only get the average-weighted effect size without any predictors/moderators. We specify the variance of our effect sizes, the data, and the method, even though REML is the default.\r\nNow, let’s take a look at the results.\r\n\r\n\r\nsummary(re_model)\r\n\r\n\r\n\r\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\r\n\r\n  logLik  deviance       AIC       BIC      AICc \r\n-22.6274   45.2548   49.2548   51.1437   50.0048   \r\n\r\ntau^2 (estimated amount of total heterogeneity): 0.4914 (SE = 0.1756)\r\ntau (square root of estimated tau^2 value):      0.7010\r\nI^2 (total heterogeneity / total variability):   94.27%\r\nH^2 (total variability / sampling variability):  17.46\r\n\r\nTest for Heterogeneity:\r\nQ(df = 19) = 145.3048, p-val < .0001\r\n\r\nModel Results:\r\n\r\nestimate      se    zval    pval   ci.lb   ci.ub \r\n  0.9429  0.1646  5.7271  <.0001  0.6202  1.2656  *** \r\n\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nThere is quite a bit of output here, but I’ll only walk through the highlights.\r\nAt the top we have k = 20, which tell us we have 20 effect sizes in the meta-analysis. \\(I^2 = 94.27\\%\\) is the variability in effect estimates that is due to heterogeneity rather than sampling error (chance). Next, we have the Q-test for heterogeneity. This is significant, suggesting that we reject the null that there is not significant heterogeneity.\r\nNext, we have our estimate. This is Hedges’ \\(g = 0.94\\), which suggests that our experimental group is nearly a full standard deviation larger than the control group. This average weighted effect size is significant, as illustrated by the p-value.\r\nModerator analyses/Meta-regression\r\nNow that we have our average weighted effect size, we are likely interested in whether any variables predict differences in that effect size across studies. In this data set, we have a variable called grade that we can use as a predictor of our average weighted effect size.\r\n\r\n\r\nre_metareg <- rma(g ~ grade, var_g, data = dat, method = 'REML')\r\n\r\n\r\n\r\nAll we had to do was, instead of having a 1 on the right side of the ~, we add the name of the variable we are interested in using to predict our effect size.\r\n\r\n\r\nsummary(re_metareg)\r\n\r\n\r\n\r\nMixed-Effects Model (k = 20; tau^2 estimator: REML)\r\n\r\n  logLik  deviance       AIC       BIC      AICc \r\n-18.3055   36.6110   42.6110   45.2821   44.3253   \r\n\r\ntau^2 (estimated amount of residual heterogeneity):     0.3398 (SE = 0.1293)\r\ntau (square root of estimated tau^2 value):             0.5829\r\nI^2 (residual heterogeneity / unaccounted variability): 91.44%\r\nH^2 (unaccounted variability / sampling variability):   11.69\r\nR^2 (amount of heterogeneity accounted for):            30.85%\r\n\r\nTest for Residual Heterogeneity:\r\nQE(df = 18) = 138.7598, p-val < .0001\r\n\r\nTest of Moderators (coefficient 2):\r\nQM(df = 1) = 8.5187, p-val = 0.0035\r\n\r\nModel Results:\r\n\r\n         estimate      se     zval    pval    ci.lb    ci.ub \r\nintrcpt    1.9637  0.3803   5.1642  <.0001   1.2184   2.7090  *** \r\ngrade     -0.3122  0.1069  -2.9187  0.0035  -0.5218  -0.1025   ** \r\n\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nWe now have the estimate of the relationship between grade and our effect size. Our \\(b =\\) -0.3121523, which means that there is a negative relationship between grade and effect size. Specifically, for every 1 unit increase in grade, the effect (i.e., difference between experimental and control groups) decreased by .31, which is pretty big!\r\nPublication bias\r\nNext, we want to examine whether publication bias is influencing our results. The rule here is that no one tool can tell you for sure whether publication bias is present. You must go by the preponderance of evidence, and remember that some approaches can be influenced by things other than publication bias.\r\nFunnel plot\r\nFirst, we will do the classic funnel plot.\r\n\r\n\r\nfunnel(re_model)\r\n\r\n\r\n\r\n\r\nWhat you’re looking for is whether there seems to be a lack of points in the left hand side of the funnel, with corresponding points on the right hand side. From this plot a few things stand out. First, it looks like we have a huge outlier on the bottom right. Second, there is maybe another outlier in the other direction, but it is the only study that has a Hedges’ g in the opposite direction. Based on this plot it isn’t super clear to me whether any publication bias is present. Technically, this would be showing small study bias, which means that smaller studies tend to have a larger effect size.\r\nEgger’s regression\r\nFunnel plots are very much a judgment call and there is some evidence that meta-analyists are not very good at spotting publication bias using them. So you’ll definitely want other approaches. Next we will run an Egger’s regression, which is essentially a meta-regression using the standard error as a predictor.\r\n\r\n\r\nregtest(re_model)\r\n\r\n\r\n\r\nRegression Test for Funnel Plot Asymmetry\r\n\r\nModel:     mixed-effects meta-regression model\r\nPredictor: standard error\r\n\r\nTest for Funnel Plot Asymmetry: z = 3.0310, p = 0.0024\r\nLimit Estimate (as sei -> 0):   b = -0.3100 (CI: -1.1610, 0.5411)\r\n\r\nThe key output here is the z-value next to test for funnel plot asymmetry. In this case, it is significant, suggesting there is a relationship between the standard error and the effect size. This suggests evidence of small-sample bias, which can be caused by publication bias.\r\nTrim and fill\r\nNext is trim and ill. The best way to think of how trim and fill works is to think of how it is related to the funnel plot. This method makes the assumption that studies on either the left or the right side of the funnel are suppressed or missing. The first part, trim,iteratively removes studies (i.e., they are “trimmed”), from one side of the plot to determine how many would need to be removed to make the funnel symmetrical. After this, it estimates new effects that are mirror images of the effects that are remaining in our plot. An adjusted average weighted effect size is then calculated.\r\n\r\n\r\ntrimfill(re_model)\r\n\r\n\r\n\r\nEstimated number of missing studies on the left side: 0 (SE = 2.0056)\r\n\r\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\r\n\r\ntau^2 (estimated amount of total heterogeneity): 0.4914 (SE = 0.1756)\r\ntau (square root of estimated tau^2 value):      0.7010\r\nI^2 (total heterogeneity / total variability):   94.27%\r\nH^2 (total variability / sampling variability):  17.46\r\n\r\nTest for Heterogeneity:\r\nQ(df = 19) = 145.3048, p-val < .0001\r\n\r\nModel Results:\r\n\r\nestimate      se    zval    pval   ci.lb   ci.ub \r\n  0.9429  0.1646  5.7271  <.0001  0.6202  1.2656  *** \r\n\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nInterestingly, our trim and fill procedure did not suggest that any studies are missing. Thus, our average weighted effect size is exactly the same as the one we calculated before.\r\nFail-safe N\r\nI’m going to be upfront and say unless a reviewer is making you use this procedure, do not do this. Even then, maybe try to push back a bit and explain that fail-safe N has been shown to have limited utility and can be severely misinterpreted. I won’t go into that here but you can easily google it. That being said, in this case we will pretend we have a pesky reviewer who will not be satisfied until we calculate fail-safe N.\r\n\r\n\r\nfsn(g, var_g, data = dat)\r\n\r\n\r\n\r\nFail-safe N Calculation Using the Rosenthal Approach\r\n\r\nObserved Significance Level: <.0001\r\nTarget Significance Level:   0.05\r\n\r\nFail-safe N: 3326\r\n\r\nTo interpret these results, this procedure suggests we would need 3326 non-significant studies to make our significant average weighted effect size no longer significant.\r\nTest of excess significance\r\nThe idea behind this procedure is that it estimates the average power of the studies in your meta-analysis. Then, it figures out based on that power how many studies you would expect to be statistically significant, and compares that to how many studies you have that are significant.\r\n\r\n\r\ntes(re_model)\r\n\r\n\r\n\r\nTest of Excess Significance\r\n\r\nObserved Number of Significant Findings: 20 (out of 20)\r\nExpected Number of Significant Findings: 15.7769\r\nObserved Number / Expected Number:       1.2677\r\n\r\nEstimated Power of Tests (based on theta = 0.9429)\r\n\r\n   min      q1  median      q3     max \r\n0.5336  0.7778  0.7924  0.8018  0.9364 \r\n\r\nTest of Excess Significance: p = 0.0077 (exact test)\r\nLimit Estimate (theta_lim):  1.3262 (where p = 0.1)\r\n\r\nHere we see that we had 20 significant effects and the test estimated that we should have around 15 or 16. We also have the estimated power, which in this case was .94. Then, we see that the test of excess significance is itself significant. This means that we reject the null that we did not see more signifcant effects than expected. This suggests, there are maybe a few more significant effects than we would expect.\r\nPublication bias conclusion\r\nBased on the preponderance of evidence, is there publication bias present here? In this case, I think it would highly depend on the research area. This effect is absolutely massive. If we were expecting to see seeing this massive of an effect, then I would say that it seems there is a true effect and it is huge which is why we get flagged for publication bias on a few of our tests. But for the majority of cases in neuroscience and psychology, I would say that we have evidence of publication bias.\r\nPlotting\r\nNow it is time to make some figures. I’m not going to walk through these, but if I were writing this up for publication, this is what I would do.\r\n\r\n\r\nforest(re_model, cex = .75, \r\n       xlab = expression(\"Hedges'\" ~italic('g')), \r\n       xlim = c(-5, 10)) \r\ntext(-3.5, 22, 'Study Number', cex = 1)#add text to top left of plot\r\ntext(9, 22, expression(italic(\"g \") * \"[95% CI]\"), pos=2)#add text to top right\r\n\r\n\r\n\r\nfunnel(re_model, xlab = expression(\"Hedges'\" ~italic('g')))#create funnel plot\r\n\r\n\r\n\r\nregplot(re_metareg)\r\n\r\n\r\n\r\n\r\nNow I don’t like the plot of our meta-regression, so I’m going to code one in ggplot and use Florida State University colors.\r\n\r\n\r\n#one fancy regression plot with ggplot2 because I don't like the regplot\r\npredicted <- predict(re_metareg)#get 95% CI's for regression line\r\nplot_dat <- cbind(dat, predicted)#add output from predict to our data. \r\nplot_dat$weights <- weights(re_metareg)#add weights for each study to our data\r\n\r\n#calculating regression line y coordinates at min and max by hand\r\ny_min <- (re_metareg$b[2]*min(plot_dat$grade)) + re_metareg$b[1]\r\ny_max <- (re_metareg$b[2]*max(plot_dat$grade)) + re_metareg$b[1]\r\n\r\n#creating the plot with three geoms: segment, which is the line, ribbon, which \r\n#shows the 95% ci, and point, showing our actual data. \r\nplot <- ggplot(data = plot_dat, aes(x = grade, y = g)) + \r\n   geom_segment(aes(x = min(plot_dat$grade), y = y_min, \r\n                    xend = max(plot_dat$grade), yend = y_max), \r\n                color = '#CEB888', size = 1)+ \r\n  geom_ribbon(aes(ymin = ci.lb, ymax = ci.ub, x = grade), \r\n              alpha = .5, fill = '#CEB888') +\r\n  geom_point(color = '#782F40', aes(size = weights))\r\n\r\n\r\n\r\n\r\n\r\n#Finishing touches\r\nplot + labs(x = 'Grade', y = expression(\"Hedges'\" ~italic('g'))) + \r\n  theme_classic(base_size = 20)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-11-16-mademo/mademo_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-11-18T12:20:27-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
